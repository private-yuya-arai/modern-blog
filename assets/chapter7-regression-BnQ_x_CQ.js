const r=`---\r
slug: "regression-chapter7"\r
title: "単回帰・重回帰分析：データを線で説明する予測の要"\r
date: "2025-10-28"\r
category: "統計学"\r
tags: ["統計検定準一級", "Python", "R", "回帰分析", "最小二乗法", "決定係数"]\r
excerpt: "データ分析の花形、回帰分析。変数が1つの単回帰から、複数の重回帰、そしてマルチコ（多重共線性）の落とし穴まで。予測モデルの第一歩を踏み出しましょう。"\r
image: "/images/regression.png"\r
---\r
\r
## この知識はいつ使うの？\r
\r
*   **売上予測**: 「広告費を100万円増やしたら、売上はいくら伸びるか？」を係数として知りたいとき。\r
*   **要因分析**: 店の売上に効いているのは「駅からの距離」なのか「店内の広さ」なのか、影響度を比較したいとき。\r
*   **機械学習**: 線形回帰モデルは、最もシンプルかつ解釈しやすい予測モデルのベースラインとなる。\r
\r
## 回帰分析のイメージ：点をつなぐ線\r
\r
回帰分析とは、散らばったデータ点の間を貫く「もっとも当てはまりの良い直線（または平面）」を引くことです。\r
\r
$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\epsilon $$\r
\r
*   $y$: 目的変数（予測したいもの。売上など）\r
*   $x$: 説明変数（原因となるもの。広告費など）\r
*   $\\beta$: 偏回帰係数（影響力の強さ。これを求めたい！）\r
\r
\`\`\`mermaid\r
graph LR\r
    Input["説明変数 x<br>(広告費・気温・距離)"] --> Model["回帰モデル<br>y = ax + b"]\r
    Model --> Output["目的変数 y<br>(売上高)"]\r
    \r
    subgraph 中身の評価\r
    Coeff["偏回帰係数<br>(傾き)"]\r
    R2["決定係数 R2<br>(当てはまりの良さ)"]\r
    Freq["p値<br>(係数は偶然じゃない？)"]\r
    end\r
    \r
    Model -.-> Coeff\r
    Model -.-> R2\r
    Model -.-> Freq\r
\`\`\`\r
\r
## 単回帰 vs 重回帰\r
\r
| 種類 | 式のイメージ | 説明 | 注意点 |\r
| :--- | :--- | :--- | :--- |\r
| **単回帰分析** | $y = ax + b$ | 説明変数が1つだけ。散布図に直線を引くイメージ。 | 現実には原因が1つということは稀。 |\r
| **重回帰分析** | $y = a_1 x_1 + a_2 x_2 + b$ | 説明変数が複数。要素ごとの影響力を分離できる。 | **多重共線性 (Multico)** に気をつける必要あり。 |\r
\r
### ⚠️ マルチコ（多重共線性）の罠\r
\r
重回帰分析で、相関係数が非常に高い変数同士（例：「気温」と「不快指数」）を一緒に説明変数に入れると、計算がおかしくなり、係数がめちゃくちゃな値になる現象です。\r
**「似たもの同士は片方だけ使う」**のが鉄則です。\r
\r
## Pythonでの実装：重回帰分析\r
\r
家賃の予測を想定してみましょう。「広さ」と「築年数」から家賃を予測します。\r
\`scikit-learn\` を使えば簡単です。\r
\r
\`\`\`python\r
import pandas as pd\r
from sklearn.linear_model import LinearRegression\r
\r
# ダミーデータ\r
data = {\r
    'rent': [80000, 120000, 95000, 150000, 70000],\r
    'size': [25, 40, 30, 50, 20],   # 広さ(m2)\r
    'age':  [10, 5, 15, 2, 30]      # 築年数(年)\r
}\r
df = pd.DataFrame(data)\r
\r
# 説明変数(X)と目的変数(y)\r
X = df[['size', 'age']]\r
y = df['rent']\r
\r
# モデルの学習\r
model = LinearRegression()\r
model.fit(X, y)\r
\r
print(f"切片: {model.intercept_:.0f}")\r
print(f"広さの係数: {model.coef_[0]:.0f} (1m2増えるとこれだけ家賃UP)")\r
print(f"築年数の係数: {model.coef_[1]:.0f} (1年古くなるとこれだけ家賃DOWN)")\r
\`\`\`\r
\r
## Rでの実装：詳細な統計量\r
\r
\`lm\` 関数を使えば、p値や決定係数($R^2$)まで詳細なレポートが出せます。実務での分析にはこちらが便利です。\r
\r
\`\`\`r\r
# mtcarsデータセットを使用\r
# mpg(燃費) を wt(重さ) と hp(馬力) で説明する\r
model <- lm(mpg ~ wt + hp, data = mtcars)\r
\r
summary(model)\r
\`\`\`\r
\r
出力結果で見るべきポイント：\r
1.  **Estimate**: 係数の値（プラスなら正の影響、マイナスなら負の影響）。\r
2.  **Pr(>|t|)**: p値。星マーク(**\\***)がついていれば、「その変数は統計的に有意に効いている」という証拠。\r
3.  **Multiple R-squared**: 決定係数。1に近いほど予測精度の高いモデル。\r
\r
## まとめ\r
\r
*   まずは**散布図**を描いて関係を見る。\r
*   **重回帰分析**を使えば、「他の条件を一定にした場合の、その変数の純粋な影響力」がわかる（ceteris paribus）。\r
*   決定係数 $R^2$ だけで判断せず、変数の有意性（t検定）やマルチコ（VIF）もチェックするプロの視点を持とう。\r
`;export{r as default};
