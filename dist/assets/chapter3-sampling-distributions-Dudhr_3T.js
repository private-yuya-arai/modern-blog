const n=`---\r
slug: "sampling-distributions-chapter3"\r
title: "標本分布と中心極限定理：統計学の「心臓部」を理解する"\r
date: "2025-10-10"\r
category: "統計学"\r
tags: ["統計検定準一級", "Python", "R", "中心極限定理", "大数の法則"]\r
excerpt: "なぜ「一部のデータ（標本）」から「全体（母集団）」がわかるのか？統計的推測を支える最強の定理、「大数の法則」と「中心極限定理」を直感的にマスターします。"\r
image: "/images/sampling.png"\r
---\r
\r
## この知識はいつ使うの？\r
\r
*   **アンケート分析**: 1000人の調査結果から、日本人全体の傾向を推測したいとき。\r
*   **品質管理**: 工場の製品をいくつか抜き取って検査し、ライン全体の品質を保証したいとき。\r
*   **A/Bテスト**: クリック率の差が「偶然」なのか「実力」なのか判断したいとき（基礎となる考え方）。\r
\r
## 統計的推測の仕組み：全体像\r
\r
統計学のゴールは「神のみぞ知る**母集団**」の性質を、手元の「**標本**」から言い当てることです。\r
\r
\`\`\`mermaid\r
graph LR\r
    God(("母集団")) -->|サンプリング| Data["標本データ"]\r
    Data -->|計算| Stat["統計量<br>平均など"]\r
    Stat -->|推測| God\r
    \r
    subgraph 見えない世界\r
    God\r
    end\r
    \r
    subgraph 見える世界\r
    Data\r
    Stat\r
    end\r
\`\`\`\r
\r
この「推測」を支えるのが、次の2つの強力な定理です。\r
\r
## 1. 2つの偉大な定理\r
\r
| 定理名 | 一言でいうと？ | 何が嬉しいの？ |\r
| :--- | :--- | :--- |\r
| **大数の法則** | データが増えれば、平均値は**真の値**に近づく。 | 「データをたくさん集めれば、真実にたどり着ける」ことが保証される。 |\r
| **中心極限定理 (CLT)** | データが増えれば、平均値の分布は**正規分布**に近づく。 | 元のデータがどんな変な形でも、合計や平均さえとれば**正規分布として扱って計算してOK**になる。 |\r
\r
### 中心極限定理のすごさ（イメージ）\r
\r
元の分布が「サイコロ」でも「偏ったコイン」でも、何度も足し合わせると、その合計値のヒストグラムはきれいな釣鐘型（正規分布）になります。\r
\r
\`\`\`mermaid\r
graph TD\r
    Dist1["一様分布"] -->|足し合わせる| Sum["合計値"]\r
    Dist2["指数分布"] -->|足し合わせる| Sum\r
    Dist3["変な分布"] -->|足し合わせる| Sum\r
    Sum --> Result["正規分布になる！"]\r
\`\`\`\r
\r
## 2. 標本分布の性質\r
\r
「標本平均 $\\bar{X}$」もまた、確率変数です（サンプリングするたびに値が変わるから）。\r
この「統計量の分布」のことを**標本分布**と呼びます。\r
\r
*   期待値 $E[\\bar{X}] = \\mu$ （母平均と同じ）\r
*   分散 $V[\\bar{X}] = \\frac{\\sigma^2}{n}$ （母分散の $1/n$ になる！）\r
\r
**ポイント**: サンプルサイズ $n$ が大きくなると、分散が小さくなり、平均値の周りにギュッと集まるようになります。だから推定の精度が上がるのです。\r
\r
## Pythonでの実験：中心極限定理を見てみよう\r
\r
「指数分布（右肩下がりの分布）」からデータを取って、その平均値の分布を描いてみます。\r
元の分布は正規分布ではありませんが、平均をとると正規分布に近づくでしょうか？\r
\r
\`\`\`python\r
import numpy as np\r
import matplotlib.pyplot as plt\r
\r
# 設定\r
N_samples = 10000  # シミュレーション回数\r
n = 30             # サンプルサイズ（これが増えると正規分布に近づく）\r
\r
# 指数分布から n 個ずつデータを取って平均を計算 × N_samples回\r
means = [np.mean(np.random.exponential(scale=1.0, size=n)) for _ in range(N_samples)]\r
\r
# ヒストグラムの描画\r
plt.figure(figsize=(8, 5))\r
plt.hist(means, bins=50, density=True, alpha=0.7, color='purple', label='Sample Means')\r
\r
# 理論的な正規分布（平均1, 分散1/n）\r
x = np.linspace(0.4, 1.6, 100)\r
pdf = (1 / np.sqrt(2 * np.pi * (1/n))) * np.exp(- (x - 1)**2 / (2 * (1/n)))\r
plt.plot(x, pdf, 'r-', lw=2, label='Theoretical Normal Dist')\r
\r
plt.title(f"Central Limit Theorem (n={n})")\r
plt.legend()\r
plt.show()\r
\`\`\`\r
\r
ヒストグラム（紫）が理論的な正規分布（赤線）ときれいに重なります。これが中心極限定理の威力です。\r
\r
## Rでの実験：大数の法則\r
\r
サンプルサイズ $n$ を増やしていくと、標本平均が徐々に真の平均（ここでは定数0）に収束していく様子を描きます。\r
\r
\`\`\`r\r
set.seed(123)\r
n_max <- 1000\r
x <- rnorm(n_max, mean=0, sd=1) # 標準正規分布から1000個生成\r
\r
# 累積平均を計算 (n=1の平均, n=2までの平均, ...)\r
cumulative_means <- cumsum(x) / (1:n_max)\r
\r
plot(cumulative_means, type="l", col="blue", lwd=2,\r
     ylim=c(-1, 1), xlab="Sample Size n", ylab="Sample Mean",\r
     main="Law of Large Numbers")\r
abline(h=0, col="red", lty=2) # 真の平均\r
\`\`\`\r
\r
$n$ が小さいときは暴れていますが、右に行くにつれて赤線（真の値）にピタリと吸い寄せられていくのがわかります。\r
\r
## まとめ\r
\r
*   **大数の法則**: 数こそ正義。データ数を増やせば真の平均に近づく。\r
*   **中心極限定理**: 足せば正規分布。この定理のおかげで、いろいろな検定や区間推定が可能になる。\r
*   どんなデータでも、とりあえず数が揃えば正規分布の道具が使えるようになる、という**統計学の免罪符**のような存在です。\r
`;export{n as default};
